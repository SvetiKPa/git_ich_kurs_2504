

05.08.25 в 9 утра произошел сбой в работе веб сервера MediaWiki. При входе по url-адресу 3.69.26.193 
видим сообщение, что в файле LocalSettings.php не задана переменная $wgServer в файле LocalSettings.php

Чтобы разобраться в проблеме, подключаемся при помощи ssh - ключа к веб-серверу. 
командой 
ssh -i /путь/к/ssh_моему_ключу user@3.69.26.193
ssh -i (путь к моему ssh-ключу) (имя сервера@ip-адрес), где -i обозначает приватный ключ. 

Проверяем, активен ли сервис командой 
sudo service httpd status  либо systemctl status httpd
Сервер активен и работает 21 час, можем попробовать перезапустить его командой 
sudo service httpd restart   
Данное действие не помогло запустить сервис. 

Попытаемся найти файл настроек LocalSettings.php. 
После подключения на сервер, из домашней директории проверим, какие файлы у нас есть, командой 
ls -la  (выводит подробный список файлов и папок в текущей директории, включая скрытые файлы, ключик -а) 

В списке файлов есть резервная копия файла настроек. Можем скопировать этот файл. При вводе команды копирования 
получаем ошибку, что нет места на устройстве. 

Нам необходимо проверить почему нет места, какие самые большие файлы и возможно их удалить. 
df -h  (disk filesystem, -h human показывает свободное место на диске в единицах (не байтах) )
Данная команда показала, что занято 10Гигабайт из имеющихся 10 Гигабайт. Свободного места на диске нет.

Находим самые большие файлы командой 
find / -type f -size +100M (ищем, начиня  с корневого раздела тип файлы, размеров больше 100 Мб)
Все команды запускаем от имени sudo (super user Do), т.к. права остальных пользователей ограничены. 

Повторяем команду поиска файлов, размер которых больше 100 Мб, но для 
каждого найденного файла ({}) выполняем команду du -h (показывает размер) и + в конце означает, 
что файлы передаются du пачкой
find / -type f -size +100M -exec du -h {} +

В папке /var/log  видим архив размером 185Мб и файл логов рамером 7Гигабайт. 
Пробуем удалить эти файлы командой 
sudo rm -rf /var/log/имя_файла  (remove -rf  рекурсивное удаления файлов и папок без подтверждения). 
Теоретически файлы логов не несут смысловой нагрузки, но данная команда является небезопасной, т.к. 
удаляет файлы практически мгновенно и без возможности восстановления (корзины в Линуксе нет). Важно не 
ошибиться в написании пути к удаляемым файлам. 

Получили 7 Гб свободного места и можем скопировать резервный файл настроек командой 
sudo cp Localsettings5.php /var/www/html/mediawiki/LocalSettings.php 
(данная команда cp copy, затем имя копируемого файла, затем путь, куда копируем, и имя нового файла).
ls -la проверяем, что файл скопирован. 

После обновления файла настроек необходимо перезагрузить сервис и проверить его статус командами, 
соответственно: 
sudo systemctl restart httpd
sudo systemctl status httpd


Переходим на наш сайт, но опять обнаружили ошибку.
Нас переадресовывают на другой ip-адрес.

Проверяем файл настроек доступным редактором, ищем переменную $wgServer. 
vi /var/www/html/mediawiki/LocalSettings.php
В режиме редактирования файла (i) исправляем "испорченный" адрес на необходимый 
$wgServer = "https://3.69.26.193"
Сохраняем файл (ESC, затем  Shift+Z Z )

Снова необходимо перезагрузить сервис и проверить его статус командами, 
соответственно: 
sudo systemctl restart httpd
sudo systemctl status httpd

Все хорошо. Сервер работает. Но необходимо разобраться с причиной, как формируются архивы логов. 
После запуска команды ls -la обнаружили, что архивы формируются каждую минуту. Это может делать 
crontab. Проверяем текущие запланированные крон-задания командой   
sudo crontab -l 

* * * * * tar -czf /var/log/httpd/log_$(date+\%Y\%m\%d).tar.gz -C /var/log/httpd .

Данная строка означает, что делается архив каждую минуту (* * * * * ) 
tar -czf  с ключами	-c создать архив, -z сжать gzip, -f указать имя файла
/var/log/httpd/log_$(date+\%Y\%m\%d).tar.gz	Имя архива с датой в формате ГГГГММДД 
например, log_20250804.tar.gz по пути  /var/log/httpd/
-C /var/log/httpd     -C  временно сменить директорию на /var/log/httpd перед выполнением архивации.
Без этого флага tar искал бы файлы относительно того места, откуда запущен cron.
. (точка в конце) добавить в архив всё содержимое текущей директории

Таким образом создается архив в папке /var/log/httpd  и опять через минуту архивируется вся директория
вместе с архивом. 
Вот причина отсутствия места на диске. 
Необходимо исправить задание в crontab.

Останавливаем ежеминутную архивацию. Для этого вызываем 
crontab -e
и коментируем # (решеткой) строку архивации. 
Выходим из редактора vi  с записью файла.
Удаляем все созданные архивы командой 
sudo rm -rf /var/log/httpd/*.tar.gz

Теперь создадим скрипт /home/ec2-user/backup_log_http.sh на создание архива один раз в сутки. 
Следующим образом:

#!/bin/bash

LOG_DIR="/var/log/httpd"
BACKUP_DIR="/home/ec2-user/backup"
DATE=$(date +%Y%m%d-%H%M)
ARCHIVE="$BACKUP_DIR/Loghttp-$DATE.tar.gz"

mkdir -p "$BACKUP_DIR"

sudo tar -czf $ARCHIVE $LOG_DIR

sudo systemctl stop httpd
#sleep 5
sudo rm -rf "$LOG_DIR/access_log"
#sleep 5
sudo systemctl start httpd

find "$BACKUP_DIR" -type f -name "Loghttp*.gz" -mtime +3 -exec rm -rf {} \;

###########################################################################

Рассмотрим подробно данный скрипт.
#!/bin/bash  - первая строка скрипта (ше-банг и путь к интерпретатору для запуска скрипта)

Далее создаем переменные, которые будем использовать далее в скрипте
LOG_DIR="/var/log/httpd"                           - директория, откуда берем архивируемые данные 
BACKUP_DIR="/home/ec2-user/backup"                 - директория, где создадутся архивы 
DATE=$(date +%Y%m%d-%H%M)                          - текущая дата+время 
ARCHIVE="$BACKUP_DIR/Loghttp-$DATE.tar.gz"         - имя архива 

mkdir -p "$BACKUP_DIR"	- создаем директорию для архивов (-р проверка, если ее нет)
 
sudo tar -czf $ARCHIVE $LOG_DIR - создаем архив, указываем сначала имя архива, а потом из какой директории 
все архивируем. 

sudo rm -rf "$LOG_DIR/access_log"  - удаляем рекурсивно без подтверждения файл логов за день. 
Перед удалением останавливаем сервис httpd на сервере, удаляем и опять запускаем сервис. 
Без этого мы не можем удалить файл логов, потому что он постоянно перезаписывается в автоматическом режиме
и "занят" сервером.

И последнее ищем в папке, где хранятся архивы ($BACKUP_DIR) файлы вида Loghttp*.gz, изменённые 
более 3 дней назад (-mtime +3),
find "$BACKUP_DIR" -type f -name "Loghttp*.gz" -mtime +3 -exec rm -rf {} \;
и удаляем их 
-exec	выполняем команду для каждого найденного файла,
rm -rf	удалияем файл (рекурсивно и без подтверждения).
{}	заменяется на имя найденного файла. \;	Завершает команду -exec (экранированная ; означает конец команды.)


Данный скрипт backup_log_http.sh делаем исполняемым файлом командой 
chmod 744 backup_log_http.sh
Проверяем и помещаем его в кронтаб для вызова в 3 часа ночи командой 
0 3 * * * /home/ec2-user/backup_log_http.sh

Подводим итоги. В ходе восстановления работоспособности веб-сервиса было выявлено, что рекурсивно ежеминутно 
создавался архив логов, который через короткое время "съедал" все свободное место на диске. Это приводило к сбоям 
в работе веб-сервиса. Данная проблема решена, исправлены ошибки при создании архива. Архив создается корректно и 
один раз в сутки. Работоспособность веб-сервиса восстановлена, место на сервере не съедается бесполезными архивами, 
архивация логов настроена и архивы хранятся 3 дня. 

В дальнейшем можно настроить Logrotate — это утилита в Linux для автоматического ротации (перезаписи), 
сжатия и удаления старых лог-файлов. 

